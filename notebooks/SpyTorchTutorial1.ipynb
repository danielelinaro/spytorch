{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Training a spiking neural network with surrogate gradients\n",
    "\n",
    "Friedemann Zenke (https://fzenke.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more details on surrogate gradient learning, please see: \n",
    "> Neftci, E.O., Mostafa, H., and Zenke, F. (2019). Surrogate Gradient Learning in Spiking Neural Networks.\n",
    "> https://arxiv.org/abs/1901.09948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The last months have seen a surge of interest in training spiking neural networks to do meaningful computations. On the one hand, this surge was fueled by the limited accomplishment of more traditional, and often considered more biologically plausible, learning paradigms in creating functional neural networks that solve interesting computational problems. This limitation was met by the undeniable success of deep neural networks in acing a diversity of challenging computational problems. A success that has raised both the bar and the question of how well this progress would translate to spiking neural networks.\n",
    "\n",
    "The rise of deep learning over the last decade is in large part due to GPUs and their increased computational power, growing training data sets, and --- perhaps most importantly --- advances in understanding the quirks and needs of the error back-propagation algorithm. For instance, we now know that we have to avoid vanishing and exploding gradients, a feat that can be accomplished by choice of a sensible nonlinearity, proper weight initialization, and a suitable optimizer. Powerful software packages supporting auto-differentiation have since made mangling with deep neural networks a breeze in comparison to what it used to be. This development begs the question of how much of this knowledge gain from deep learning and its tools we can leverage to train spiking neural networks. Although a complete answer to these questions cannot be given at the moment, it seems that we can learn a lot.\n",
    "\n",
    "In this tutorial, we use insights and tools from machine learning to build, step-by-step, a spiking neural network. Explicitly, we set out with the goal of building networks that solve (simple) real-world problems. To that end, we focus on classification problems and use supervised learning in conjunction with the aforementioned back-propagation algorithm. To do this, we have to overcome a vanishing gradient problem caused by the binary nature of the spikes themselves.\n",
    "\n",
    "In this tutorial, we will first show how a simple feed-forward spiking neural network of leaky integrate-and-fire (LIF) neurons with current-based synapses can be formally mapped to a discrete-time recurrent neural network (RNN). We will use this formulation to explain why gradients vanish at spikes and show one way of how the problem can be alleviated. Specifically, we will introduce surrogate gradients and provide practical examples of how they can be implemented in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping LIF neurons to RNN dynamics\n",
    "\n",
    "The de-facto standard neuron model for network simulations in computational neuroscience is the LIF neuron model which is often formally written as a time continuous dynamical system in differential form:\n",
    "$$\\tau_\\mathrm{mem} \\frac{\\mathrm{d}U_i^{(l)}}{\\mathrm{d}t} = -(U_i^{(l)}-U_\\mathrm{rest}) + RI_i^{(l)}$$\n",
    "where $U_i$ is the membrane potential of neuron $i$ in layer $l$, $U_\\mathrm{rest}$ is the resting potential, $\\tau_\\mathrm{mem}$ is the membrane time constant, $R$ is the input resistance, and $I_i$ is the input current. The membrane potential $U_i$ characterizes the hidden state of each neuron and, importantly, it is not directly communicated to downstream neurons. However, a neuron fires an action potential or spike at the time $t$ when its membrane voltage exceeds the firing threshold $\\vartheta$. After having fired a spike, a neurons membrane voltage is reset $U_i \\rightarrow U_\\mathrm{rest}$. We write\n",
    "$$S_i^{(l)}(t)=\\sum_{k \\in C_i^l} \\delta(t-t_j^k)$$ \n",
    "for the spike train (ie. the sum of all spikes $C_i^l$ emitted by neuron $i$ in layer $l$). Here $\\delta$ is the Dirac delta function and $t_i^k$ are the associated firing times of the neuron.\n",
    "\n",
    "Spikes travel down the axon and generate a postsynaptic currents in connected neurons. Using our above formalism we can thus write\n",
    "$$\\frac{\\mathrm{d}I_i}{\\mathrm{d}t}= -\\frac{I_i(t)}{\\tau_\\mathrm{syn}} + \\sum_j W_{ij} S_j^{(0)}(t) + \\sum_j V_{ij} S_j^{(1)}(t)$$\n",
    "where we have introduced the synaptic weight matrices $W_{ij}$ (feed-forward), $V_{ij}$ (recurrent), and the synaptic decay time constant $\\tau_\\mathrm{syn}$.\n",
    "\n",
    "To link to RNNs apparent, we will now express the above equations in discrete time. In the interest of brevity we switch to natural units $U_\\mathrm{rest}=0$, $R=1$, and $\\vartheta=1$. Our arguments remain unaffected by this choice, and all results can always be re-scaled back to physical units. To highlight the nonlinear character of a spike, we start by noting that we can set\n",
    "$$S_i^{(l)}(t)=\\Theta(U_i^{(l)}(t)-\\vartheta)$$\n",
    "where $\\Theta$ denotes the Heaviside step function.\n",
    "\n",
    "Assuming a small simulation time step of $\\Delta_t>0$ we can approximate the synaptic dynamics by\n",
    "$$I_i^{(l)}(t+1) = \\alpha I_i^{(l)}(t) + \\sum_j W_{ij} S_j^{(l-1)}(t) +\\sum_j V_{ij} S_j^{(l)}(t)$$\n",
    "with the constant $\\alpha=\\exp\\left(-\\frac{\\Delta_t}{\\tau_\\mathrm{syn}} \\right)$. Further, the membrane dynamics can be written as\n",
    "$$U_i^{(l)}(t+1) = \\underbrace{\\beta U_i^{(l)}(t)}_{\\mathrm{leak}} + \\underbrace{I_i^{(l)}(t)}_{\\mathrm{input}} -\\underbrace{S_i^{(l)}(t)}_{\\mathrm{reset}}$$\n",
    "with the output $S_i(t) = \\Theta(U_i(t)-1)$ and the constant $\\beta=\\exp\\left(-\\frac{\\Delta_t}{\\tau_\\mathrm{mem}}\\right)$. Note the distinct terms on the right-hand-side of the equation which are responsible individually for i) leak, ii) synaptic input, and iii) the spike reset.\n",
    "\n",
    "\n",
    "\n",
    "These equations can be summarized succinctly as the computational graph of an RNN with a specific connectivity structure. \n",
    "<img src=\"figures/snn_graph/snn_graph.png\" width=\"450\">\n",
    "Time flows from left to right. Inputs enter the network at each time step from the bottom of the graph ($S_i^{(0)}$). These inputs sequentially influence the synaptic currents $I_i^{(1)}$, membrane potentials the $U_i^{(1)}$, and finally the spiking output $S_i^{(1)}$.  Moreover, dynamic quantities have direct input on future time steps. We have suppressed the indices $i$ in the figure for clarity.\n",
    "\n",
    "The computational graph illustrates a concept which is known as unrolling in time, which emphasizes the duality between a deep neural network and a recurrent neural network, which is nothing more but a deep network in time (with tied weights). Due to this fact, we can train RNNs using the back-propagation of error through time (BPTT). We will discuss problems arising from the binary character of the spiking nonlinearity later. For now, let us start by implementing the above dynamics in a three-layer spiking neural network in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple multilayer network model with a single hidden layer, as shown below. For simplicity, we will not use recurrent connections $V$ for now, keeping in mind that they can be added later should the need arise.\n",
    "\n",
    "<img src=\"figures/mlp_sketch/mlp_sketch.png\">\n",
    "\n",
    "For the sake of argument, we set the numbers for the input, hidden and output neurons as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_inputs  = 100\n",
    "nb_hidden  = 4\n",
    "nb_outputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, we are technically simulating an RNN. Thus we have to simulate our neurons for a certain number of timesteps. We will use 1ms timesteps, and we want to simulate our network for say 200 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 1e-3\n",
    "nb_steps  = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take advantage of parallelism, we will set up our code to work on batches of data like this is usually done for neural networks that are trained in a supervised manner.\n",
    "To that end, we specify a batch size here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these basic design choices made, we can now start building the actual network. Here we will be using PyTorch, but you will be able to reproduce these results in most common machine learning libraries.\n",
    "\n",
    "We start by importing the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Uncomment the line below to run on GPU\n",
    "# device = torch.device(\"cuda:0\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple synthetic dataset \n",
    "\n",
    "We start by generating some random spiking data set, which we will use as input to our network. In the beginning, we will work with a single batch of data. It will be straight forward to expand later what we have learned to larger datasets.\n",
    "\n",
    "Suppose we want our network to classify a set of different sparse input spike trains into two categories. \n",
    "\n",
    "To generate some synthetic data, we fill a tensor of (batch_size x nb_steps x nb_inputs) with random uniform numbers between 0 and 1 and use this to generate our input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 5 # Hz\n",
    "prob = freq*time_step\n",
    "mask = torch.rand((batch_size,nb_steps,nb_inputs), device=device, dtype=dtype)\n",
    "x_data = torch.zeros((batch_size,nb_steps,nb_inputs), device=device, dtype=dtype, requires_grad=False)\n",
    "x_data[mask<prob] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the spike raster of the first input pattern, this synthetic dataset looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = 0\n",
    "fig,ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "ax.imshow(x_data[data_id].cpu().t(), cmap=plt.cm.gray_r, aspect=\"auto\")\n",
    "ax.set_xlabel(\"Time (ms)\")\n",
    "ax.set_ylabel(\"Unit\")\n",
    "sns.despine()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assign a random label of 0 or 1 to each of our input patterns. Our network's task will be to differentiate these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = torch.tensor(1*(np.random.rand(batch_size)<0.5), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no structure in the data (because it is entirely random). Thus we won't worry about generalization now and only care about our ability to overfit these data with the spiking neural network we are going to build in a jiffy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the spiking network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to implement our LIF neuron model in discrete time.\n",
    "We will first do this step by step before we wrap all the steps into a function later on.\n",
    "But first, we fix several model constants such as the membrane and the synaptic time constant. Moreover, we define some essential variables, including our $\\alpha$ and $\\beta$ as described above. We do this now because we will use some of these variables to scale our weights to meaningful ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our weight matrices, which connect the input and the hidden layer, as well as the matrix connecting the hidden layer with the output layer. Moreover, we initialize these weights randomly from a normal distribution. Note that we scale the variance with the inverse square root of the number of input connections. Moreover, for the sake of simplicity, we ignore Dale's law in this tutorial. Thus weights can be either excitatory or inhibitory. This choice is prevalent in artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
    "\n",
    "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 10\n",
    "hidden_id = 2\n",
    "inp = (x_data[batch_id, :, :] @ w1[:, hidden_id]).detach().numpy()\n",
    "fig,ax = plt.subplots(1, 1, figsize=(5,3))\n",
    "ax.plot(inp, 'k', lw=1)\n",
    "for side in 'right','top':\n",
    "    ax.spines[side].set_visible(False)\n",
    "ax.set_xlabel('Time (a.u.)')\n",
    "ax.set_ylabel(f'Input to hidden neuron #{hidden_id}')\n",
    "ax.set_title(f'Batch #{batch_id}')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A spiking neuron model in discrete time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do to implement our spiking neuron is to multiply all input spikes with the weight matrix. We have to do this for each time step in each input example in the batch. Because we have stored our input spikes in a rank three tensor we can express this operation in a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = torch.einsum(\"abc,cd->abd\", (x_data, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These \"weighted\" input spikes will now feed into our synaptic variable and, ultimately, the membrane potential. To trigger a spike, we need to define moreover a threshold or spike function, which we do in the following. We will later have to alter this definition to train the network, but more about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spiking nonlinearity (the naive way)\n",
    "\n",
    "In discrete-time, as explained earlier, we can formulate our spiking nonlinearity as a Heaviside step function. So let's begin with defining a Heaviside function. One way of implementing it is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_fn(x):\n",
    "    out = torch.zeros_like(x)\n",
    "    out[x > 0] = 1.0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each trial, we initialize the synaptic currents and membrane potentials at zero.\n",
    "Next, we need to implement a loop that simulates our neuron models over time. \n",
    "Moreover, we will record the membrane potentials and output spikes of all trials and all neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "# Here we define two lists which we use to record the membrane potentials and output spikes\n",
    "mem_rec = []\n",
    "spk_rec = []\n",
    "\n",
    "# Here we loop over time\n",
    "for t in range(nb_steps):\n",
    "    mthr = mem-1.0\n",
    "    out = spike_fn(mthr)\n",
    "    rst = out.detach() # We do not want to backprop through the reset\n",
    "\n",
    "    new_syn = alpha*syn + h1[:,t]\n",
    "    #new_mem = (beta*mem +syn)*(1.0-rst)\n",
    "    new_mem = beta*mem + syn - rst\n",
    "    \n",
    "    mem_rec.append(mem)\n",
    "    spk_rec.append(out)\n",
    "    \n",
    "    mem = new_mem\n",
    "    syn = new_syn\n",
    "\n",
    "# Now we merge the recorded membrane potentials into a single tensor\n",
    "mem_rec = torch.stack(mem_rec,dim=1)\n",
    "spk_rec = torch.stack(spk_rec,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_rec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. The above loop has now simulated our neurons for '''nb_steps''' and stored their membrane traces and output spikes. Let us take a look at those membrane potentials in which we directly \"paste\" the spikes for visual inspection. We will directly plot multiple trials at once and define a little helper function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5, w=1.5, h=1.25):\n",
    "    fig,ax = plt.subplots(dim[0], dim[1], figsize=(dim[1]*w, dim[0]*h), sharex=True, sharey=True)\n",
    "    if spk is not None:\n",
    "        dat = 1.0*mem\n",
    "        dat[spk>0.0] = spike_height\n",
    "        dat = dat.detach().cpu().numpy()\n",
    "    else:\n",
    "        dat = mem.detach().cpu().numpy()\n",
    "    for i in range(dim[0]):\n",
    "        for j in range(dim[1]):\n",
    "            k = i*dim[1] + j\n",
    "            ax[i,j].plot(dat[k], lw=1)\n",
    "            ax[i,j].grid(which='major', axis='y', lw=0.5, ls=':', color=[.6,.6,.6])\n",
    "            ax[i,j].axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_voltage_traces(mem_rec, spk_rec)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our random initialization gives us some sporadic spiking. Thus far, we have only an input layer and a spiking layer, which should become our hidden layer. Next, we will have to add a readout layer to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a readout layer\n",
    "\n",
    "To use our network as a classifier, we need to have a readout layer on whose output we can define a cost function. There are several possibilities for doing this. For instance, we could count output layer spikes, or we could directly define an objective function on the membrane potential of the output neurons. Here we will follow the latter approach, but keep in mind that there are many other possibilities of defining an output layer and respective cost functions on them.\n",
    "\n",
    "In the following, we will build the output layer as a population of leaky integrator neurons. The reason for this choice is that leaky integration is the natural way of how neurons receive the spiking output of their brethren. Moreover, because we will need this code again, we combine our code from above plus the added readout layer into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn(inputs):\n",
    "    h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden layer activity\n",
    "    for t in range(nb_steps):\n",
    "        mthr = mem-1.0\n",
    "        out = spike_fn(mthr)\n",
    "        rst = out.detach() # We do not want to backprop through the reset\n",
    "\n",
    "        new_syn = alpha*syn + h1[:,t]\n",
    "        #new_mem = (beta*mem +syn)*(1.0-rst)\n",
    "        new_mem = beta*mem + syn - rst\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "        \n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec,dim=1)\n",
    "    spk_rec = torch.stack(spk_rec,dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out]\n",
    "    for t in range(nb_steps):\n",
    "        new_flt = alpha*flt +h2[:,t]\n",
    "        new_out = beta*out +flt\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out)\n",
    "\n",
    "    out_rec = torch.stack(out_rec,dim=1)\n",
    "    other_recs = [mem_rec, spk_rec]\n",
    "    return out_rec, other_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run this code and plot the output layer \"membrane potentials\" below. As desired, these potentials do not have spikes riding on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rec,other_recs = run_snn(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_voltage_traces(out_rec)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By preventing the output neurons from spiking themselves, we can define a relatively smooth objective on their membrane voltages directly. Specifically, we use the maximum voltage over time of each output unit\n",
    "$$\\hat U^\\mathrm{out}_i=\\max_t U^\\mathrm{out}_i(t)$$\n",
    "and then use this vector as input for either an argmax to compute the classification accuracy or as we will see below as input for a standard softmax function in conjunction with a negative log-likelihood loss for optimizing the weights in the network. \n",
    "\n",
    "Let us first compute the classification accuracy of this random network. We will see that this accuracy is somewhere around 50% as it should be since that corresponds to the chance level of our synthetic task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_accuracy():\n",
    "    \"\"\" Dirty little helper function to compute classification accuracy. \"\"\"\n",
    "    output,_ = run_snn(x_data)\n",
    "    m,_= torch.max(output,1) # max over time\n",
    "    _,am=torch.max(m,1) # argmax over output units\n",
    "    acc = np.mean((y_data==am).detach().cpu().numpy()) # compare to labels\n",
    "    print(\"Accuracy %.3f\"%acc)\n",
    "    \n",
    "print_classification_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "So far, we have built the infrastructure to simulate our spiking neural network, but we have worked with purely random network weights thus far.\n",
    "The vanilla method to adjust network weights to decrease the specified objective is gradient descent. \n",
    "Machine learning libraries like Tensorflow and PyTorch make implementing gradient descent a breeze.\n",
    "We first perform gradient descent on the correct gradient and use this as a motivation for introducing surrogate gradients.\n",
    "Here we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning with the true gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [w1,w2] # The paramters we want to optimize\n",
    "optimizer = torch.optim.Adam(params, lr=2e-3, betas=(0.9,0.999)) # The optimizer we are going to use\n",
    "\n",
    "log_softmax_fn = nn.LogSoftmax(dim=1) # The log softmax function across output units\n",
    "loss_fn = nn.NLLLoss() # The negative log likelihood loss function\n",
    "\n",
    "# The optimization loop\n",
    "loss_hist = []\n",
    "for e in range(1000):\n",
    "    # run the network and get output\n",
    "    output,_ = run_snn(x_data) \n",
    "    # compute the loss\n",
    "    m,_=torch.max(output,1)\n",
    "    log_p_y = log_softmax_fn(m) \n",
    "    loss_val = loss_fn(log_p_y, y_data)\n",
    "\n",
    "    # update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # store loss value\n",
    "    loss_hist.append(loss_val.item())\n",
    "    \n",
    "loss_hist_true_grad = loss_hist # store for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We appreciate that loss decreases over iterations and converges towards a steady state. The classification accuracy, however, does not seem to improve dramatically throughout the optimization. What a shame! \n",
    "\n",
    "The underlying reason is that the nonlinearity of the hidden units have zero derivatives everywhere except at threshold crossings, where they become infinite. In practice that means that weight updates in the hidden layer vanish and the weights remain unmodified. By plotting the hidden layer activations and comparing them with what we have plotted before, we will see that these activations have not changed at all. Thus no learning happens in the hidden layer. The reason why the loss decreased initially during optimization is that the output layer weights could still change and allow for some improvement (even if it was very little).\n",
    "\n",
    "To improve performance, we need to get the hidden layer units to take part in learning. To achieve this, we will introduce a surrogate gradient in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,other_recordings = run_snn(x_data)\n",
    "mem_rec, spk_rec = other_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_voltage_traces(mem_rec, spk_rec)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_voltage_traces(output)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning with surrogate gradients\n",
    "\n",
    "In the last section, we saw that the hidden layer units did not participate.\n",
    "The underlying reason is that the partial derivative of the step function we used has a vanishing derivative everywhere (except at zero where it becomes infinite).\n",
    "\n",
    "Most conventional neural networks avoid this problem by choosing a nonlinearity with non-zero partial derivative. For instance, sigmoidal or tanh units were standard during the beginnings of neural networks research. Today, ReLUs are more common. Importantly, all these activation functions have substantial non-zero support, which allows gradients to flow (to a greater or lesser extent).\n",
    "\n",
    "What do we if we want to stick to our binary nonlinearity? There have been several approaches to tackle this problem. Here we use one such strategy which has been applied successfully to spiking neural networks: We use a surrogate gradient approach.\n",
    "\n",
    "The idea behind a surrogate gradient is dead simple. Instead of changing the nonlinearity itself, we only change the gradient. Thus we use a different \"surrogate\" gradient to optimize parameters that would otherwise have a vanishing gradient.\n",
    "\n",
    "<img src=\"figures/surrgrad/surrgrad.png\" width=\"450\">\n",
    "Specifically, we use the partial derivative of a function which to some extent approximates the stepfunction $\\Theta(x)$.\n",
    "In what follows, chiefly, we will use (up to rescaling) the partial derivative of a fast sigmoid function $\\sigma(x)$. \n",
    "While $\\Theta$ is invariant to multiplicative rescaling, $\\sigma$ isn't. Thus we have to introduce a scale parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "    \n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(inp)\n",
    "        out = torch.zeros_like(inp)\n",
    "        out[inp > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        inp, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input / (SurrGradSpike.scale * torch.abs(inp)+1.0)**2\n",
    "        return grad\n",
    "    \n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which\n",
    "# implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following lines will reinitialize the weights\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [w1,w2]\n",
    "optimizer = torch.optim.Adam(params, lr=2e-3, betas=(0.9,0.999))\n",
    "\n",
    "log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "loss_hist = []\n",
    "for e in range(1000):\n",
    "    output,_ = run_snn(x_data)\n",
    "    m,_=torch.max(output,1)\n",
    "    log_p_y = log_softmax_fn(m)\n",
    "    loss_val = loss_fn(log_p_y, y_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_val.backward()\n",
    "    optimizer.step()\n",
    "    loss_hist.append(loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3.3,2),dpi=150)\n",
    "plt.plot(loss_hist_true_grad, label=\"True gradient\")\n",
    "plt.plot(loss_hist, label=\"Surrogate gradient\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,other_recordings = run_snn(x_data)\n",
    "mem_rec, spk_rec = other_recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_voltage_traces(mem_rec, spk_rec)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_voltage_traces(output)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,_ = run_snn(x_data)\n",
    "m,_=torch.max(output,1)\n",
    "\n",
    "# Compute training accuracy\n",
    "_,am=torch.max(m,1)\n",
    "acc = np.mean((y_data==am).detach().cpu().numpy())\n",
    "print(\"Accuracy %f\"%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
